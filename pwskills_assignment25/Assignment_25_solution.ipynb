{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c3c981",
   "metadata": {},
   "source": [
    "#### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c969d",
   "metadata": {},
   "source": [
    "Baye's theorem is a method to determine conditional probabilities, that is probabilities of even A occurring given that another event has already occured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f6d86",
   "metadata": {},
   "source": [
    "#### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bbf50",
   "metadata": {},
   "source": [
    "P(A | B) = P(B | A) * P(A) / P(B)\n",
    "\n",
    "where P(A | B) is the probability of event A occurring given that event B has occurred, P(B | A) is the probability of event B occurring given that event A has occurred, P(A) is the prior probability of event A, and P(B) is the prior probability of event B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09f5876",
   "metadata": {},
   "source": [
    "#### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa4b58",
   "metadata": {},
   "source": [
    "- Medical Diagnosis\n",
    "- Spam Filtering \n",
    "- Weather Forecasting\n",
    "- Stock Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c028298e",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc9865",
   "metadata": {},
   "source": [
    "Bayes' theorem is closely related to conditional probability. Conditional probability is the probability of an event A occurring, given that another event B has occurred. It is denoted as P(A | B), which reads as \"the probability of A given B.\" Bayes' theorem provides a way to calculate conditional probabilities by reversing the conditioning. Specifically, it states that:\n",
    "\n",
    "P(A | B) = P(B | A) * P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e17a4",
   "metadata": {},
   "source": [
    "#### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e3a88",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes: This classifier assumes that the features follow a normal (Gaussian) distribution. It is often used when the features are continuous, and their distribution is not too skewed.\n",
    "\n",
    "Multinomial Naive Bayes: This classifier is used when the features are discrete, such as word counts in text classification. It assumes that the features follow a multinomial distribution.\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is similar to the Multinomial Naive Bayes, but it is used when the features are binary (i.e., present or absent). It assumes that the features follow a Bernoulli distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e54752",
   "metadata": {},
   "source": [
    "#### Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3 \n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db2601",
   "metadata": {},
   "source": [
    "To predict the class of the new instance using Naive Bayes, we need to calculate the posterior probabilities of each class given the values of X1=3 and X2=4. Since we have equal prior probabilities for each class, the class with the highest posterior probability will be the predicted class.\n",
    "\n",
    "We can calculate the posterior probabilities for each class as follows:\n",
    "\n",
    "P(A | X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A) * P(A) / P(X1=3, X2=4)\n",
    "\n",
    "P(B | X1=3, X2=4) = P(X1=3 | B) * P(X2=4 | B) * P(B) / P(X1=3, X2=4)\n",
    "\n",
    "Using the Naive Bayes assumption of feature independence, we can calculate the likelihoods of each feature value given the class as follows:\n",
    "\n",
    "P(X1=3 | A) = 4/10 = 0.4\n",
    "P(X1=3 | B) = 1/6 ≈ 0.1667\n",
    "\n",
    "P(X2=4 | A) = 3/10 = 0.3\n",
    "P(X2=4 | B) = 1/3 ≈ 0.3333\n",
    "\n",
    "To calculate the marginal probability of the feature values, we can sum over the product of the likelihoods and the prior probabilities for each class:\n",
    "\n",
    "P(X1=3, X2=4) = P(X1=3 | A) * P(X2=4 | A) * P(A) + P(X1=3 | B) * P(X2=4 | B) * P(B)\n",
    "= 0.4 * 0.3 * 0.5 + 0.1667 * 0.3333 * 0.5\n",
    "≈ 0.0633 + 0.0278\n",
    "≈ 0.0911\n",
    "\n",
    "Therefore, we can calculate the posterior probabilities as:\n",
    "\n",
    "P(A | X1=3, X2=4) = 0.4 * 0.3 * 0.5 / 0.0911 ≈ 0.6596\n",
    "P(B | X1=3, X2=4) = 0.1667 * 0.3333 * 0.5 / 0.0911 ≈ 0.3404\n",
    "\n",
    "The class with the highest posterior probability is A, so the Naive Bayes classifier would predict that the new instance belongs to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
