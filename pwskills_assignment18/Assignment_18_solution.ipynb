{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b189f8",
   "metadata": {},
   "source": [
    "#### Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c4e17",
   "metadata": {},
   "source": [
    "The purpose of grid search CV (cross-validation) is to find the best combination of hyperparameters that maximizes the performance of a model. \n",
    "\n",
    "The grid search algorithm performs the following steps:\n",
    "\n",
    "1. Define a hyperparameter space: the user defines a set of hyperparameters and the range of values that each hyperparameter can take. This defines a hyperparameter space.\n",
    "\n",
    "2. Create a grid: all possible combinations of hyperparameters are created from the hyperparameter space. This results in a grid.\n",
    "\n",
    "3. Train and evaluate the model for each combination of hyperparameters: the model is trained and evaluated using cross-validation for each combination of hyperparameters in the grid.\n",
    "\n",
    "4. Select the best set of hyperparameters: the combination of hyperparameters that results in the best performance is selected.\n",
    "\n",
    "5. Test the model: the final selected model is then tested on a separate test set to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e29f78",
   "metadata": {},
   "source": [
    "#### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af759382",
   "metadata": {},
   "source": [
    "| Grid search cv | Randomize search cv |\n",
    "| --- | --- |\n",
    "| We try every combination of a present list of values of the hyper-parameters and choose the best combination based on the cross validation score. | Tries random combinations of a range of values (we have to define the number of iterations). It is good at testing a wide range of values and normally it reaches a very good combination very fast |\n",
    "|  It takes a lot of time to fit (because it will try all the combinations) | very fast as compare to grid search |\n",
    "| Gives us the best combination of hyperparameters | It doesn't guarantee that we have the best parameters |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b5bcb",
   "metadata": {},
   "source": [
    "#### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbcbeb",
   "metadata": {},
   "source": [
    "Data leakage (or leakage) happens when your training data contains information about the target, but similar data will not be available when the model is used for prediction. This leads to high performance on the training set (and possibly even the validation data), but the model will perform poorly in production.\n",
    "\n",
    "In other words, leakage causes a model to look accurate until you start making decisions with the model, and then the model becomes very inaccurate.\n",
    "\n",
    "Suppose we have a dataset of customer purchases at a grocery store, and we want to build a model to predict which customers are likely to buy bananas. One of the features in the dataset is the customer's favorite fruit, which is recorded after the purchase is made.\n",
    "\n",
    "However, including the favorite fruit as a feature in the model would result in data leakage, as the model would have access to information that is only available after the purchase is made. If we were to use this feature in the model, it would result in a model that would overfit to the training data and would not perform well on new data, because the model would not have access to the customer's favorite fruit at the time of the purchase.\n",
    "\n",
    "To avoid data leakage, we should exclude the customer's favorite fruit from the dataset and only use features that are available at the time of the purchase, such as the total amount of the purchase, the time of day, the customer's age, and so on. This would result in a more accurate model that would generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e70b6",
   "metadata": {},
   "source": [
    "#### Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef9738",
   "metadata": {},
   "source": [
    "There are several ways to prevent the data leakage when building machine learning.\n",
    "1. Separate your data into training, validation, and testing\n",
    "2. Use cross-validation techniques\n",
    "3. Data augmentation\n",
    "4. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94719ab8",
   "metadata": {},
   "source": [
    "#### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125e6ce",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. It shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) produced by the model.\n",
    "\n",
    "| Actual | Positive | Actual Negative |\n",
    "| --- | --- | --- |\n",
    "| Predicted Positive | \tTrue Positive (TP) | False Positive (FP) |\n",
    "| Predicted Negative |\tFalse Negative (FN)| True Negative (TN) |\n",
    "\n",
    "* Accuracy: The proportion of correct predictions out of the total number of predictions made by the model. It is calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "* Precision: The proportion of true positives out of the total number of predicted positives. It is calculated as TP / (TP + FP).\n",
    "\n",
    "* Recall (also called sensitivity or true positive rate): The proportion of true positives out of the total number of actual positives. It is calculated as TP / (TP + FN).\n",
    "\n",
    "* Specificity (also called true negative rate): The proportion of true negatives out of the total number of actual negatives. It is calculated as TN / (TN + FP).\n",
    "\n",
    "* F1-score: A weighted average of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f39015",
   "metadata": {},
   "source": [
    "#### Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd0589",
   "metadata": {},
   "source": [
    "| Precision | Recall |\n",
    "| --- | --- |\n",
    "| Precision measures the proportion of true positives out of the total number of predicted positives. In other words, it measures how precise the model's positive predictions are. | Recall, on the other hand, measures the proportion of true positives out of the total number of actual positives. In other words, it measures how well the model is able to identify all positive cases |\n",
    "| Precision = TP / (TP + FP) | Recall = TP / (TP + FN) |\\\n",
    "\n",
    "\n",
    "To understand the difference between precision and recall, consider the following example:\n",
    "\n",
    "Suppose we have a model that predicts whether or not a patient has a disease. In this case, a true positive (TP) is when the model correctly predicts that a patient has the disease, and a false positive (FP) is when the model predicts that a patient has the disease, but they actually do not have it. A false negative (FN) is when the model predicts that a patient does not have the disease, but they actually do have it. Finally, a true negative (TN) is when the model correctly predicts that a patient does not have the disease.\n",
    "\n",
    "In this context, precision measures how many of the patients predicted to have the disease actually have it. High precision means that the model is good at identifying patients with the disease, and it has a low rate of false positives.\n",
    "\n",
    "Recall, on the other hand, measures how many of the actual positive cases the model is able to correctly identify. High recall means that the model is good at identifying all patients with the disease, and it has a low rate of false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab37ee",
   "metadata": {},
   "source": [
    "#### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b718292",
   "metadata": {},
   "source": [
    "Here's an example of a confusion matrix for a binary classification problem:\n",
    "| Actual | Positive | Actual Negative |\n",
    "| --- | --- | --- |\n",
    "| Predicted Positive | \tTrue Positive (TP) | False Positive (FP) |\n",
    "| Predicted Negative |\tFalse Negative (FN)| True Negative (TN) |\n",
    "\n",
    "- True Positive (TP): The model predicted a positive class, and the actual class was positive.\n",
    "\n",
    "- False Positive (FP): The model predicted a positive class, but the actual class was negative.\n",
    "\n",
    "- False Negative (FN): The model predicted a negative class, but the actual class was positive.\n",
    "\n",
    "- True Negative (TN): The model predicted a negative class, and the actual class was negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd3ccc",
   "metadata": {},
   "source": [
    "#### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86240d8c",
   "metadata": {},
   "source": [
    "These are common metrics that can be derived from confusion matrix:\n",
    "* Accuracy: The proportion of correct predictions out of the total number of predictions made by the model. It is calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "* Precision: The proportion of true positives out of the total number of predicted positives. It is calculated as TP / (TP + FP).\n",
    "\n",
    "* Recall (also called sensitivity or true positive rate): The proportion of true positives out of the total number of actual positives. It is calculated as TP / (TP + FN).\n",
    "\n",
    "* Specificity (also called true negative rate): The proportion of true negatives out of the total number of actual negatives. It is calculated as TN / (TN + FP).\n",
    "\n",
    "* F1-score: A weighted average of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fefe0",
   "metadata": {},
   "source": [
    "#### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558e94c",
   "metadata": {},
   "source": [
    "Here's how the values in a confusion matrix relate to the accuracy of a model:\n",
    "\n",
    "True Positive (TP): A higher number of true positives increases the accuracy of the model because it means that the model is correctly identifying more positive cases.\n",
    "\n",
    "False Positive (FP): A higher number of false positives decreases the accuracy of the model because it means that the model is incorrectly identifying more negative cases as positive.\n",
    "\n",
    "False Negative (FN): A higher number of false negatives decreases the accuracy of the model because it means that the model is incorrectly identifying more positive cases as negative.\n",
    "\n",
    "True Negative (TN): A higher number of true negatives increases the accuracy of the model because it means that the model is correctly identifying more negative cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9afcd",
   "metadata": {},
   "source": [
    "#### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a501cc6",
   "metadata": {},
   "source": [
    "Class imbalance: If the number of samples in one class is much larger than the other class, the model may be biased towards the majority class, resulting in poor performance on the minority class. You can identify this by looking at the number of true positives, false positives, true negatives, and false negatives in each class and checking for significant differences.\n",
    "\n",
    "Misclassification patterns: You can examine the confusion matrix to see which types of errors the model is making. For example, if the model is frequently misclassifying a particular class, you may need to adjust your features or model to better capture that class.\n",
    "\n",
    "Overfitting: If the model performs well on the training data but poorly on the test data, it may be overfitting to the training data. In this case, you may see high accuracy on the training set, but poor performance on the test set. You can use the confusion matrix to identify whether the model is making different types of errors on the training and test sets.\n",
    "\n",
    "Data quality: The quality of the data used to train the model can have a significant impact on its performance. You can examine the confusion matrix to identify whether certain types of errors are more prevalent, which may indicate issues with the data quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
