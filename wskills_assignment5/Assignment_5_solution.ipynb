{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b74c173",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5e62b1",
   "metadata": {},
   "source": [
    "Min-Max scaling also known as normalization used to rescaling the range of features to scale the range in [0,1] .\n",
    "\n",
    "it is used to preserver original shape of original distribution. it does'nt meaningfully change the information embedded in the oringinal data.\n",
    "\n",
    "This scaling technique is useful in situations where the values of different features in a dataset have different scales or units, making it difficult to compare them. Min-Max scaling ensures that all features have the same range and scale, making it easier to analyze and compare them.\n",
    "\n",
    "For example, suppose we have a dataset that contains the heights of individuals in centimeters and their weights in kilograms. The height values range from 150 cm to 190 cm, while the weight values range from 50 kg to 100 kg. We can apply Min-Max scaling to both features, so that they both fall in the range of 0 to 1.\n",
    "\n",
    "Suppose we want to scale the height feature to a range of 0 to 1. We can apply the Min-Max scaling formula as follows:\n",
    "\n",
    "scaled_height = (height - min_height) / (max_height - min_height)\n",
    "\n",
    "where min_height is the minimum height value in the dataset (150 cm), and max_height is the maximum height value in the dataset (190 cm). Let's say we want to scale the height value of 170 cm. The scaled height value will be:\n",
    "\n",
    "scaled_height = (170 - 150) / (190 - 150) = 0.33\n",
    "\n",
    "Similarly, we can apply Min-Max scaling to the weight feature, so that it falls in the range of 0 to 1. After scaling both features, they will be normalized and can be easily compared and analyzed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3adf87",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259ec4f",
   "metadata": {},
   "source": [
    "It involves scaling the data so that each data point is represented by a vector of unit length (i.e., a magnitude of 1) in the same direction as the original vector. In other words, it scales the data to have a unit norm (magnitude).\n",
    "\n",
    "This technique is useful when the scale of the features varies widely, and we want to give equal importance to all the features. It helps in avoiding the dominance of any particular feature in the model.\n",
    "\n",
    "One of the main differences between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling does not transform the data to a specific range but instead scales the data to a unit length. Additionally, Unit Vector scaling does not preserve the distribution of the original data as it only changes the magnitude of the vectors and not their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6b51b",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86d24d",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a widely used technique for dimensionality reduction in machine learning and data science. It is a statistical method that reduces the dimensionality of a high-dimensional dataset while retaining most of the original information\n",
    "\n",
    "PCA is useful when dealing with high-dimensional datasets, where the number of features is large compared to the number of observations. It helps to reduce the dimensionality of the dataset and to remove the correlations among the features, making the data easier to analyze and visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f7dc10",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed657b",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used for feature extraction, which is the process of extracting new features from existing features in a dataset. In feature extraction, we select a subset of the original features that capture the most important information in the data. PCA can help us in this process by identifying the principal components that explain the most variance in the data and using them as new features.\n",
    "\n",
    "For example, suppose we have a dataset with 10 features, and we want to reduce the dimensionality of the dataset to 3 features for building a machine learning model. We can apply PCA for feature extraction as follows:\n",
    "\n",
    "Standardize the data by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "Compute the covariance matrix of the standardized data.\n",
    "\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "Choose the top 3 eigenvectors (principal components) that explain most of the variance in the data.\n",
    "\n",
    "Transform the data into the new 3-dimensional space by multiplying it with the chosen eigenvectors.\n",
    "\n",
    "Use the transformed data as the new features for building a machine learning model.\n",
    "\n",
    "The resulting transformed data contains 3 features, which capture the most important information in the original dataset. These new features can be used for building machine learning models, such as regression or classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3dc63",
   "metadata": {},
   "source": [
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60860cc7",
   "metadata": {},
   "source": [
    "We clearly know that we have three features price, rating, time and these feature have different unit and it become difficult for us to analyze and visualize beacause the unit are different so we use normalization min max scaling.\n",
    "\n",
    "example:\n",
    "\n",
    "For example, suppose the dataset contains the following values for the price, rating, and delivery time features:\n",
    "\n",
    "Price: [10, 20, 15, 25, 30]\n",
    "Rating: [2.5, 3.5, 4.0, 3.0, 4.5]\n",
    "Delivery Time: [30, 45, 20, 60, 40]\n",
    "\n",
    "To scale these features using Min-Max scaling, we would perform the following steps:\n",
    "\n",
    "Identify the numerical features to be scaled: Price, Rating, and Delivery Time.\n",
    "\n",
    "Determine the minimum and maximum values for each of the features:\n",
    "\n",
    "Price: minimum = 10, maximum = 30\n",
    "\n",
    "Rating: minimum = 2.5, maximum = 4.5\n",
    "\n",
    "Delivery Time: minimum = 20, maximum = 60\n",
    "\n",
    "Apply the Min-Max scaling formula to scale the features:\n",
    "\n",
    "Price: [(10-10)/(30-10), (20-10)/(30-10), (15-10)/(30-10), (25-10)/(30-10), (30-10)/(30-10)] = [0.00, 0.33, 0.17, 0.67, 1.00]\n",
    "\n",
    "Rating: [(2.5-2.5)/(4.5-2.5), (3.5-2.5)/(4.5-2.5), (4.0-2.5)/(4.5-2.5), (3.0-2.5)/(4.5-2.5), (4.5-2.5)/(4.5-2.5)] = [0.00, 0.50, 0.75, 0.25, 1.00]\n",
    "\n",
    "Delivery Time: [(30-20)/(60-20), (45-20)/(60-20), (20-20)/(60-20), (60-20)/(60-20), (40-20)/(60-20)] = [0.25, 0.75, 0.00, 1.00, 0.50]\n",
    "\n",
    "Replace the original feature values with the scaled values in the dataset. The resulting dataset with the scaled values would be:\n",
    "\n",
    "Price: [0.00, 0.33, 0.17, 0.67, 1.00]\n",
    "\n",
    "Rating: [0.00, 0.50, 0.75, 0.25, 1.00]\n",
    "\n",
    "Delivery Time: [0.25, 0.75, 0.00, 1.00, 0.\n",
    "\n",
    "As you can see we scale all the features in range of 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c3652",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b8aa7",
   "metadata": {},
   "source": [
    "As we all know all the features where in dataset very important for model but during the feature selection we loss huge amount of data which is not fit for out model so we will try to use PCA so that we can reduce the dimension preserve all the information without any huge loss in data.\n",
    "using PCA we select any number of dimesion with majority variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b50e8",
   "metadata": {},
   "source": [
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6b8fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c394d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c420dbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.fit_transform([[1, 5, 10, 15, 20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1e9cd",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa3585",
   "metadata": {},
   "source": [
    "In this case, since we have 5 features, we can have at most 5 principal components. However, we can use PCA to determine how many components are needed to explain the majority of the variance in the data.\n",
    "\n",
    "but as you can see we have 5 feature and 3 are most important that is height, weight, age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cf4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
