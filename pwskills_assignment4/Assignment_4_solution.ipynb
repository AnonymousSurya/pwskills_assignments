{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9f361da-568f-47dc-99e9-f16c04a34265",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c24d4f-d4a0-4c7e-bc51-dfe64e44e125",
   "metadata": {},
   "source": [
    "#### Probability Mass Function (PMF) and Probability Density Function (PDF) are two fundamental concepts in probability theory and statistics used to describe the probability distribution of a random variable.\n",
    "\n",
    "#### Probability Mass Function (PMF):\n",
    "> The PMF is a function that maps each possible value of a discrete random variable to its probability of occurrence. It represents the probability that a random variable takes a certain value. The sum of probabilities of all possible values of the random variable is equal to 1. The PMF is defined as:\n",
    "\n",
    "- PMF(x) = P(X = x)\n",
    "\n",
    "> where X is a random variable, x is a possible value that X can take, and P(X = x) is the probability that X takes the value x.\n",
    "\n",
    "> Example: Consider the roll of a fair six-sided die. The possible values that the die can take are 1, 2, 3, 4, 5, and 6, each with a probability of 1/6. The PMF of the random variable X, which represents the number on the top face of the die, is:\n",
    "\n",
    "- PMF(1) = 1/6, PMF(2) = 1/6, PMF(3) = 1/6, PMF(4) = 1/6, PMF(5) = 1/6, PMF(6) = 1/6\n",
    "\n",
    "#### Probability Density Function (PDF):\n",
    "> PDF is a function that describes the probability density of a continuous random variable. It represents the probability that a random variable falls within a certain range of values. The area under the curve of the PDF between any two points represents the probability that the random variable falls within that range. The integral of the PDF over the entire domain is equal to 1. The PDF is defined as:\n",
    "\n",
    "- PDF(x) = dF(x)/dx\n",
    "\n",
    "> Example: Consider a normal distribution with a mean of 0 and a standard deviation of 1. The PDF of the random variable X, which represents the values that X can take, is:\n",
    "\n",
    "- PDF(x) = 1/(sqrt(2pi)) exp(-(x^2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e012176-293d-425a-987a-c394e5bee9e5",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a872e2-e9ae-43fa-9d91-048e9b602a9c",
   "metadata": {},
   "source": [
    "#### The Cumulative Distribution Function (CDF) is a fundamental concept in probability theory and statistics that describes the probability that a random variable takes a value less than or equal to a given value. It is defined for both discrete and continuous random variables.\n",
    "\n",
    "#### For a discrete random variable X, the CDF is defined as:\n",
    "\n",
    "- F(x) = P(X ≤ x)\n",
    "\n",
    "> where x is any real number, and P(X ≤ x) is the probability that X takes a value less than or equal to x.\n",
    "\n",
    "> For a continuous random variable X, the CDF is defined as the integral of the PDF from negative infinity to x:\n",
    "\n",
    "- F(x) = ∫{-∞, x} PDF(t) dt\n",
    "\n",
    "> where PDF(x) is the Probability Density Function of X.\n",
    "\n",
    "> Example: Consider the roll of a fair six-sided die. The possible values that the die can take are 1, 2, 3, 4, 5, and 6, each with a probability of 1/6.\n",
    "The CDF of the random variable X, which represents the number on the top face of the die, is:\n",
    "\n",
    "- F(x) = P(X ≤ x)\n",
    "\n",
    "- = 0 for x < 1\n",
    "- = 1/6 for 1 ≤ x < 2\n",
    "- = 2/6 for 2 ≤ x < 3\n",
    "- = 3/6 for 3 ≤ x < 4\n",
    "- = 4/6 for 4 ≤ x < 5\n",
    "- = 5/6 for 5 ≤ x < 6\n",
    "- = 1 for x ≥ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d0e8bf-6e4e-4d05-a513-254fb4ef01d2",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02a7781-c198-408e-907d-1c478248a9d7",
   "metadata": {},
   "source": [
    "#### Some examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "- Heights or weights of a population: The normal distribution can be used to model the distribution of heights or weights of a population.\n",
    "- Test scores: The normal distribution can be used to model the distribution of test scores in a population.\n",
    "- Measurement errors: The normal distribution can be used to model the distribution of errors in measurements, such as errors in laboratory measurements.\n",
    "- Financial returns: The normal distribution can be used to model the distribution of financial returns, such as stock prices.\n",
    "- Time taken to complete a task: The normal distribution can be used to model the distribution of time taken to complete a task, such as the time taken to fill an order in a factory.\n",
    "- The normal distribution is a symmetric distribution, which means that the mean, median, and mode are all equal. The shape of the distribution is bell-shaped, and the total area under the curve is equal to 1. The standard normal distribution is a special case of the normal distribution with a mean of 0 and a standard deviation of 1.\n",
    "#### The normal distribution is often used as a model in statistical analysis because of its properties, such as the central limit theorem, which states that the sum of a large number of independent and identically distributed random variables will tend to a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909da70-7957-4c78-9cf2-1c87b0ab4ad9",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d660f0-f8ca-4b74-a9b6-a1d715236f51",
   "metadata": {},
   "source": [
    "#### The importance of the normal distribution can be explained as follows:\n",
    "\n",
    "- Many natural phenomena follow a normal distribution: The normal distribution is observed in many natural phenomena, such as the heights and weights of a population, the measurement errors in laboratory experiments, the test scores of students, the time taken to complete a task, and many more.\n",
    "- Statistical inference: The normal distribution is used extensively in statistical inference, which involves drawing conclusions about a population based on a sample of data. Many statistical tests, such as the t-test and ANOVA, assume that the data follow a normal distribution.\n",
    "- Central limit theorem: The central limit theorem states that the sum of a large number of independent and identically distributed random variables will tend to a normal distribution, regardless of the distribution of the individual variables. This property makes the normal distribution a fundamental concept in probability theory and statistics.\n",
    "#### Some real-life examples of the normal distribution are:\n",
    "\n",
    "- Heights of adults: The heights of adults in a population follow a normal distribution, with a mean of around 5 feet 7 inches and a standard deviation of around 3 inches.\n",
    "- Test scores: The scores on a standardized test, such as the SAT or GRE, follow a normal distribution, with a mean of around 500-600 and a standard deviation of around 100-200.\n",
    "- Body temperature: The body temperature of a healthy human follows a normal distribution, with a mean of around 98.6 degrees Fahrenheit and a standard deviation of around 0.5 degrees.\n",
    "- IQ scores: The IQ scores of a population follow a normal distribution, with a mean of 100 and a standard deviation of 15.\n",
    "- Stock market returns: The daily returns on the stock market follow a normal distribution, with a mean of around 0 and a standard deviation of around 1-2%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563a4b0-3e13-45f3-83c3-278dd07a4522",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff29874-8788-4f11-8142-57c6a577930f",
   "metadata": {},
   "source": [
    "#### The Bernoulli distribution is a discrete probability distribution that models the outcome of a single binary event. It takes a single parameter p, which represents the probability of success for the event, and outputs a probability distribution for the event's outcomes.\n",
    "\n",
    "> Example: Consider flipping a fair coin. The Bernoulli distribution models this as a binary event where the outcome is either heads or tails. If we define \"success\" as getting heads, then the probability of success is 0.5, and the probability of failure (getting tails) is also 0.5. The Bernoulli distribution would output a probability distribution where the probability of success is 0.5 and the probability of failure is 0.5.\n",
    "\n",
    "#### The key difference between the Bernoulli distribution and the binomial distribution is that the Bernoulli distribution models the outcome of a single event, while the binomial distribution models the number of successes in a fixed number of independent and identical Bernoulli trials. In other words, the binomial distribution is the sum of multiple independent Bernoulli random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9a1d6-cb58-409e-bee1-604950b2d925",
   "metadata": {},
   "source": [
    "### 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49136b1-d2aa-4bb2-8f84-1fef10b951c5",
   "metadata": {},
   "source": [
    "#### We can use the z-table to solve this question.\n",
    "\n",
    "#### The appropriate formula to use is:\n",
    "- z = (x - μ) / σ\n",
    "\n",
    "> where x is the value of the observation you are interested in (in this case, x = 60), μ is the mean of the dataset, and σ is the standard deviation of the dataset.\n",
    "\n",
    "#### Substituting the values given in the question, we get:\n",
    "- z = (60 - 50) / 10 = 1\n",
    "\n",
    "#### Now, we need to use the z-table to find the probability that a z-score is greater than 1.\n",
    "\n",
    "#### Using the z-table, we look up the probability corresponding to a z-score of 1.00 in the positive z-score column. The table tells us that the probability is 0.8413.\n",
    "\n",
    "#### The probabilty that we got from z-table is the probability of randomly selected number less than 60, because z-table gives us the probability of the values on the left side of 60. But we need the values on the right side (greater than 60) of 60. So we can get that probability by subtracting 0.8413 from 1 as:\n",
    "\n",
    "- 1 - 0.8413 = 0.1587"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37f6c4-b10c-4591-9817-9ff8dc0eb0c8",
   "metadata": {},
   "source": [
    "### 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41085164-afd5-43c9-bcb5-381c4a3cafb6",
   "metadata": {},
   "source": [
    "#### Uniform distribution, also known as a rectangular distribution, is a probability distribution where all possible outcomes are equally likely to occur. It is often used in statistics to model situations where each outcome is equally likely to occur, such as rolling a fair die or picking a card from a well-shuffled deck.\n",
    "\n",
    "> Example:\n",
    "Rolling a fair six-sided die: When rolling the die, each face has an equal probability of showing up, which is 1/6 or approximately 0.1667. This means that any number between 1 and 6 is equally likely to be rolled, and the probability of rolling any particular number is 1/6.\n",
    "\n",
    "#### In the above examples, the probability density function of the uniform distribution is constant over the entire range of possible outcomes. That is, the probability of any particular outcome is proportional to the size of the range of possible outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f48ee-bbcb-480d-87b6-eb4bc4e861d9",
   "metadata": {},
   "source": [
    "### 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c372bb-f53f-4975-baac-8a268a914e51",
   "metadata": {},
   "source": [
    "#### The z-score is a statistical measure that expresses how far a data point is from the mean of a distribution in terms of standard deviations.\n",
    "\n",
    "#### The formula for calculating the z-score of a data point is:\n",
    "- z = (x - μ) / σ\n",
    "> where x is the data point, μ is the mean of the distribution, and σ is the standard deviation.\n",
    "\n",
    "#### Importance:\n",
    "\n",
    "> The z-score is important because it allows us to standardize data from different distributions, which can then be compared and analyzed more easily. By converting data into z-scores, we can compare observations from different samples or populations and make meaningful statements about their relative positions.\n",
    "\n",
    "> The z-score is also useful in hypothesis testing, where it is used to calculate the probability of observing a value as extreme as the one observed, assuming a certain null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2d572-2ea9-4dba-90fe-e7f66256d57a",
   "metadata": {},
   "source": [
    "### 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1836456-8280-4bca-9fca-6cd209ff8937",
   "metadata": {},
   "source": [
    "#### The Central Limit Theorem (CLT) is a fundamental result in probability theory and statistics that describes the behavior of the sum or average of a large number of independent and identically distributed random variables. It states that, under certain conditions, the sum or average of such variables will converge to a normal distribution, regardless of the distribution of the individual variables.\n",
    "\n",
    "#### The significance of the CLT is that it provides a theoretical foundation for many statistical techniques that assume normally distributed data. For example, many hypothesis tests and confidence intervals rely on the assumption of normality, which is often justified by the CLT.\n",
    "#### Additionally, the CLT is important in practical applications such as quality control, where it is often necessary to estimate the mean and variance of a population based on a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76a4a3-9de5-4656-8d14-b73fc27ff07e",
   "metadata": {},
   "source": [
    "### 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3590e07-e5c3-4287-bef2-96a615f7e8ea",
   "metadata": {},
   "source": [
    "#### The assumptions of the Central Limit Theorem are:\n",
    "\n",
    "- Independence: The observations in the sample are independent of each other, meaning that the outcome of one observation does not influence the outcome of another observation.\n",
    "- Sample size: The sample size is sufficiently large. The larger the sample size, the better the approximation to the normal distribution.\n",
    "- Identically distributed: The sample data comes from a population that has a well-defined mean and variance. The observations in the sample are identically distributed, meaning that they come from the same population.\n",
    "- Finite variance: The population has a finite variance. This assumption ensures that the sample variance is also finite.\n",
    "- Non-skewed population distribution: The population distribution is not strongly skewed. A strongly skewed population distribution can affect the validity of the Central Limit Theorem, and a larger sample size may be required to approximate a normal distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
